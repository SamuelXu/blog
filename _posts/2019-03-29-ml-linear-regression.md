---
layout: post
title:  "线性回归算法梳理"
date:   2019-03-29 21:05:00 +0800
categories: machine 
tags: machine
---


## 一、机器学习一些概念

**监督学习**：在样本中有目标变量或者预测目标的学习方法，也就是从有标记的训练数据来进行学习的方法。比如归类和回归问题。监督学习解决的问题是：对于输入数据x，目标变量y是多少

**无监督学习**：目标变量不存在的情况下，从数据中学习的方法。比如聚类方法。这里解决的问题是：从数据x中能发现什么？比如：构成x的最佳数据簇是哪6个？或者x中哪三个特征最频繁共现？

**泛化能力**：机器学习的算法对新样本的适应能力。用来描述：在学习集学习到的规律，在学习集以外的训练集上面是否也能给出合适输出的能力。一定程度上可以描述出算法的正确性。

**过拟合**：为了得到一致性假设而使假设过度严格，泛化能力差的情况。过拟合往往考虑了过多噪音，或者建立了太复杂的模型，反而并不能反映出真实规律，只能在训练集获得较好的拟合效果，在训练集以外却不能较好的拟合数据。过拟合也有可能是样本太少造成的。这是在训练集表现好，泛化能力差的情况。

**欠拟合**：在训练集表现差，泛化能力也差的模型。欠拟合是指模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好地捕捉到数据特征，不能够很好地拟合数据。

**方差**：用来描述数据离散程度的度量。可以用来描述源数据与期望值之间的偏离程度。方差度量了同样大小的训练集的变动所导致的学习性能的变化, 即 刻画了数据扰动所造成的影响.

**偏差**：期望预测与真实标记的误差成为偏差。偏差度量了学习算法的期望预测与真实结果的偏离程序, 即 刻画了学习算法本身的拟合能力 .

**交叉验证**：在实际的训练中，训练的结果对于训练集的拟合程度通常还是挺好的（初始条件敏感），但是对于训练集之外的数据的拟合程度通常就不那么令人满意了。因此我们通常并不会把所有的数据集都拿来训练，而是分出一部分来（这一部分不参加训练）对训练集生成的参数进行测试，相对客观的判断这些参数对训练集之外的数据的符合程度。这种思想就称为交叉验证。

## 二、线性回归的原理

$$y(x,w)=w_0+\sum_{j=1}^m w_j\varnothing_j(x)$$ 

其中，$$w_0$$ 为偏置参数，M为特征数目，$$\varnothing_j(x)$$ 为基函数（比如径向基函数，sigmoid基函数等）。

特别的，当$$\varnothing_j(x)=x_j​$$ 时，即为简单的多元线性回归。函数模型就成为：

$$y(x,w) = w_0 + w_1 x_1 + w_2 x_2 + ... + w_n x_n= w^T x_i = W^TX $$ 

$$X = \left[ \begin{matrix} 1 & x_1 & ... & x_n\end{matrix} \right] ^ T $$  $$W = \left[ \begin{matrix} w_0 & w_1 & ... & w_n\end{matrix} \right] ^ T$$ 

## 三、线性回归中的损失函数、代价函数、目标函数

### 损失函数

机器通过损失函数进行学习。这是一种评估特定算法对给定数据建模程度的方法。如果预测值与实际结果偏离较远，损失函数会得到一个非常大的值。在一些优化函数的辅助下，损失函数逐渐学会减少预测值的误差。损失函数一般针对单个样本，公式如下：

$$L(\theta) = |y_i - \hat y_i| =  (y_i - \hat y_i) ^2  ​$$

### 代价函数

定义在整个训练集上的，是所有样本误差的平均，也就是损失函数的平均，针对整个样本。

$$MSE =  {\sum_{i=1}^n (y_i - \hat y_i) ^2 \over n}  $$

### 目标函数

也就是最终要优化的函数，即最优化经验风险和结构风险，函数表达式为：

$$F(\theta) = {\sum_{i=1}^N |y_i - \hat y_i| \over N} + 正则化项​$$

线性回归目标函数公式为：

$$(w^*, b^*) = {1 \over 2N} \sum_{i=1} ^N (y_i - \hat y_i)^2 + \lambda \sum_{i=1}^N (\theta _j ^2) $$

## 四、优化方法

### 梯度下降法：

 **梯度下降**（Gradientdescent）是获取代价函数最小值的过程。

1. 思想

   想象现在在一座山（三维立体图形），有多个山峰和山谷（极大值和极小值）。当你在某个位置，找到最快下山的路线（偏导数最小而且是负数的方向），并走一小步，然后接着寻找最快下山的路线，直到到达最低点。

2. 存在问题

   从上述思想可知，对于有多个极小值情况下，用梯度下降算法很有可能到不了最小值点，只会到达某个极小值点，就因为周围没有减小的路线，而停止。 因此，不同的起始值，最终得到的结果会不一样。

3. 公式方法

   $$\theta = \theta - \alpha {1 \over N} \Delta _\theta L(\theta) \tag 1$$

   $$\Delta _\theta L(\theta) = {d f(\theta) \over \theta} \tag2$$

   目标函数$$L(\theta)$$关于参数 $$\theta$$ 的梯度将是目标函数上升最快的方向。对于最小化优化问题，只需要将参数沿着梯度相反的方向迈一步，就可以实现目标函数的下降，其中有一个学习率 $$\alpha$$，他决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。通过方程(2)不断优化，得到w和b

### 牛顿法：

牛顿法的思想就是通过迭代来找到函数的零点。牛顿法是二阶收敛，梯度下降是一阶收敛，牛顿法通常比梯度下降算法收敛速度要快，只需要更少的迭代次数就能获得最小值。然而一次牛顿迭代要比一次梯度下降更昂贵，因为它需要计算$${Hessan} ​$$矩阵并且求它的逆，这将花费不少时间。但是当参数个数$$n ​$$不是太大时，总体来说速度还是要快很多。牛顿法的迭代公式：

$$x_{t+1} = x_t - {f^` (x_t) \over f^`` (x_t)} \tag{一元函数} $$

$$x_{k+1} = x_k - H_k ^ {-1} g_k\tag{二元函数}  $$

### 拟牛顿法：

拟牛顿法的思想是不计算目标函数的Hessian矩阵然后求逆矩阵，而是通过其他手段得到Hessian矩阵或其逆矩阵的近似矩阵。具体做法是构造一个近似Hessian矩阵或其逆矩阵的正定对称矩阵，用该矩阵进行牛顿法的迭代。将函数在$$x_{k+1}$$点出进行泰勒展开，忽略二次以上的项，有:

$$f(x) \approx f(x_{k+1}) + \Delta f(x_{k+1})^T (x - x_{k+1}) + {1 \over 2} (x - x_{k+1}) ^T \Delta ^2 f(x_{k+1}) (x - x_{k+1})$$

对两边同时去梯度，并令$$X = X_k$$ ，有:

$$\Delta f(x_{k+1}) - \Delta f(x_k) \approx \Delta ^2 f(x_{k+1}) (x_{k+1} - x_k)$$ 简写为：

$$g_{k+1} - g_k \approx H_{k+1} (x_{k+1} - x_k) $$ 令 $$s_k = x_{k+1} - x_k$$  $$y_k = g_{k+1} - g_k$$ ， 即可简写为： 

$$y_k = H_{k+1} s_k$$， 即

$$s_k \approx H_{k+1} ^{-1} y_k$$



## 五、线性回归的评估指标

对模型的泛化性能进行评估，不仅需要有效可行的实验估计方法，还需要衡量模型泛化能力的评价标准，这就是评估指标。我们需要根据任务的需求来确定评价标准。
在预测任务中，给定样例集 $$D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)} $$，其中是$$y_i$$示例$$x_i$$的真实标记。要评估模型f的性能，就要把学习期预测结果f(x)与真实标记y进行比较。
回归任务最常用的性能度量是“均方误差”(mean squared error)：
$$E(f;D) = {1 \over m} \sum _{i=1} ^n (f(x_i) - y_i) ^2$$

更一般的，对于数据分布D和概率密度函数p(.) ，均方误差可描述为：
$$E(f;D) = \int _{x \in D} (f(x) - y) ^2 p(x) dx$$

## 六、sklearn参数详解

调用方法：
`sklearn.linear_model.LinearRegression(fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)`

参数解析：

**fit_intercept**：boolean，optional，default True 

是否计算此模型的截距。如果设置为False，则不会在计算中使用截距（例如，预计数据已经居中）。

**normalize** : boolean, optional, default False

当fit_intercept设置为False时，将忽略此参数。如果为True，则回归量X将在回归之前通过减去平均值并除以l2范数来归一化。如果您希望标准化，请在使用normalize = False的估算器调用fit之前使用sklearn.preprocessing.StandardScaler。

**copy_X** : boolean, optional, default True

如果为True，则将复制X;否则，它可能会被覆盖。

**n_jobs** : int or None, optional (default=None)

用于计算的核数。这只会为n_targets> 1和足够大的问题提供加速。除非在joblib.parallel_backend上下文中，否则表示1。 -1表示使用所有处理器。
