---
layout: post
title:  "逻辑回归算法梳理"
date:   2019-03-31 22:05:00 +0800
categories: machine 
tags: machine
---


## 一、逻辑回归的原理

逻辑回归主要解决分类问题，多用来解决二分类，也可以用来解决多分类问题，相当于预测目标变量为离散值的情况。

逻辑回归的假设是：

$$h_{\theta} (x) = g({\theta} ^TX) \tag1$$

其中g 代表逻辑函数，在逻辑回归中通常为sigmoid函数，简称S型函数：

$$g(z) = {1 \over 1+e^{-z}} \tag{sigmoid}$$

当 $$h_{\theta}>=0.5​$$ 时，预测 y=1；反之为0

## 二、逻辑回归与线性回归的联系与区别

- 逻辑回归本质上就是在线性回归的基础上，再进行了sigmoid变换，值域映射到(0,1)
- 线性回归输出为连续值，逻辑回归输出为概率值。逻辑回归通常解决分类问题。
- 线性回归采用MSE损失函数，逻辑回归采用交叉熵损失函数。

## 三、逻辑回归损失函数推导及优化

 逻辑回归的代价函数为交叉熵函数：

$$J(\theta) = {1 \over m} \sum _{i=1} ^m Cost(h_{\theta}(x^{(i)}), y^{(i)})$$ 其中，

$$ Cost(h_{\theta}(x), y) = \begin{cases} -log(h_{\theta} (x)), \quad y=1\\ -log(1 - h_{\theta} (x)) , \quad y=0  \end{cases} ​$$  (其中$${h_{\theta} (x)}​$$ 表示预测值为1的概率)

- 当y=1，误差随着$$h_{\theta}(x)$$变小而变大。当y=1 且 $$h_{\theta}(x)=1$$ 时误差为0。
- 当y=0，误差随着$$h_{\theta}(x)$$ 的变大而变大。当y=0 且 $$h_{\theta}(x)=0$$ 时代价为0。

将构建的 $$ Cost(h_{\theta}(x), y)$$ 简化如下：

$$ Cost(h_{\theta}(x), y) = -y log(h_{\theta}(x)) - (1-y) log(1 - h_{\theta}(x))$$

代入代价函数可以得到

$$J(\theta) = {1 \over m} \sum_{i=1}^m[-y^{(i)} log(h_{\theta}(x^{(i)})) - (1-y^{(i)}) log(1-h_{\theta}(x^{(i)}))]​$$



## 四、正则化与模型评估指标

$$J(\theta) = {1 \over m} \sum_{i=1}^m[-y^{(i)} log(h_{\theta}(x^{(i)})) - (1-y^{(i)}) log(1-h_{\theta}(x^{(i)}))] + {\lambda \over 2m} \sum_{j=1} ^n {\theta} _j^2$$



## 五、逻辑回归的优缺点

优点：1）适合需要得到一个分类概率的场景。2）计算代价不高，容易理解实现。LR在时间和内存需求上相当高效。它可以应用于分布式数据，并且还有在线算法实现，用较少的资源处理大型数据。3）LR对于数据中小噪声的鲁棒性很好，并且不会受到轻微的多重共线性的特别影响。（严重的多重共线性则可以使用逻辑回归结合L2正则化来解决，但是若要得到一个简约模型，L2正则化并不是最好的选择，因为它建立的模型涵盖了全部的特征。）

缺点：1）容易欠拟合，分类精度不高。2）数据特征有缺失或者特征空间很大时表现效果并不好。3）不能用 logistic 回归来解决非线性问题


## 六、样本不均衡问题解决办法

- 欠采样
- 过采样
- 阈值移动

## 七、sklearn参数详解

```sklearn.linear_model.LogisticRegression(penalty=‘l2’, dual=False,‍tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1,class_weight=None, random_state=None, solver=‘liblinear’,max_iter=100, multi_class=‘ovr’, verbose=0, warm_start=False, n_jobs=1)```

penalty	正则化选择参数，参数可选值为l1和l2，分别对应l1正则化和l2正则化，默认是l2正则化。
dual	用来指明是否将原问题改成他的对偶问题，对偶问题可以理解成相反问题，比如原问题是求解最大值的线性规划，那么他的对偶问题就是转化为求解最小值的线性规划，适用于样本较小的数据集，因样本小时，计算复杂度较低。
tol	残差收敛条件，默认是0.0001，也就是只需要收敛的时候两步只差＜0.0001就停止，可以设置更大或更小。(逻辑回归模型的损失函数是残差平方和)
C	正则化系数，正则化强度的导数，必须是一个正数，值越小，正则化强度越大，即防止过拟合的程度更大。
fit_intercept	是否将截距/方差加入到决策模型中，默认为True。
class_weight:class_weight	是很重要的一个参数，是用来调节正负样本比例的，默认是值为None，也就是正负样本的权重是一样的，你可以以dict的形式给模型传入任意你认为合适的权重比，也可以直接指定一个值“balanced”，模型会根据正负样本的绝对数量比来设定模型最后结果的权重比。
random_state	随机种子的设置，默认是None,如果设置了随机种子，那么每次使用的训练集和测试集都是一样的，这样不管你运行多少次，最后的准确率都是一样的；如果没有设置，那么每次都是不同的训练集和测试集，最后得出的准确率也是不一样的。
solver	用来指明损失函数的优化方法，默认是‘liblinear’方法。
max_iter	算法收敛的最大迭代次数，即求取损失函数最小值的迭代次数，默认是100，
multi_class	分类方法参数选择，‘ovr’和‘multinomial’两个值可以选择，默认值为‘ovr’，如果分类问题是二分类问题，那么这两个参数的效果是一样的，主要体现在多分类问题上。对于多分类问题，"ovr"分类方法是：针对每一类别进行判断时，都会把这个分类问题简化为是/非两类问题；而‘multinomial’是从众多类别中选出两个类别，对这两个类别进行判断，待判断完成后，再从剩下的类别中再选出两类进行判断，直至最后判断完成。
verbose	英文意思是”冗余“，就是会输出一些模型运算过程中的东西（任务进程），默认是False，也就是不需要输出一些不重要的计算过程。
warm_start	是否使用上次的模型结果作为初始化，默认是False，表示不使用。
n_jobs	并行运算数量(核的数量)，默认为1，如果设置为-1，则表示将电脑的处理器全部用上。